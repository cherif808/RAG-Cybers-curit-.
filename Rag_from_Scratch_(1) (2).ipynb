{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Exemple 1 – Recherche sémantique simple (TF-IDF + cosinus + MMR)\n",
        "\n",
        "Dans ce premier exemple, nous ne faisons **pas encore de RAG complet**, mais seulement de la **recherche d’information**. Les phrases sont transformées en vecteurs TF-IDF, puis on calcule la **similarité cosinus** pour trouver les passages les plus proches d’une question. On introduit aussi **MMR (Maximal Marginal Relevance)** pour montrer comment on peut équilibrer *pertinence* et *diversité* des passages récupérés."
      ],
      "metadata": {
        "id": "VYenj5DEAiXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUpsA-0J3sTA",
        "outputId": "e0086661-0148-4fcc-bc71-84f39a8a602f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-k by cosine: [('RAG retrieves relevant passages and augments the prompt for generation.', 0.378), ('Graph neural networks generalize convolution to graph-structured data.', 0.0), ('Cosine similarity measures the angle between two vectors.', 0.0), ('FAISS is a library for efficient similarity search at scale.', 0.0)] \n",
            "\n",
            "MMR re-ranked: [('RAG retrieves relevant passages and augments the prompt for generation.', 0.378), ('Graph neural networks generalize convolution to graph-structured data.', 0.0), ('Cosine similarity measures the angle between two vectors.', 0.0)] \n",
            "\n",
            "Question: How does RAG produce better answers than a plain LLM?\n",
            "\n",
            "Grounded answer (based on retrieved context):\n",
            "Here is what we found:\n",
            "- RAG retrieves relevant passages and augments the prompt for generation.\n",
            "- Graph neural networks generalize convolution to graph-structured data.\n",
            "- Cosine similarity measures the angle between two vectors.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "docs = [\n",
        "    \"Graph neural networks generalize convolution to graph-structured data.\",\n",
        "    \"RAG retrieves relevant passages and augments the prompt for generation.\",\n",
        "    \"Cosine similarity measures the angle between two vectors.\",\n",
        "    \"FAISS is a library for efficient similarity search at scale.\",\n",
        "    \"Sentence transformers map text to dense semantic vectors.\",\n",
        "    \"Maximal Marginal Relevance (MMR) balances relevance and diversity.\"\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "doc_matrix = vectorizer.fit_transform(docs)\n",
        "\n",
        "def embed_query_tfidf(q: str):\n",
        "    return vectorizer.transform([q])\n",
        "\n",
        "def retrieve_topk(q: str, k: int = 3) -> List[Tuple[int, float]]:\n",
        "    q_vec = embed_query_tfidf(q)\n",
        "    sims = cosine_similarity(q_vec, doc_matrix).ravel()\n",
        "    idx = np.argsort(-sims)[:k]\n",
        "    return [(int(i), float(sims[i])) for i in idx]\n",
        "\n",
        "def mmr(query_vec, doc_mat, lambda_mult=0.7, top_k=3):\n",
        "\n",
        "    rel = cosine_similarity(query_vec, doc_mat).ravel()\n",
        "    selected = []\n",
        "    candidates = list(range(doc_mat.shape[0]))\n",
        "\n",
        "    while len(selected) < top_k and candidates:\n",
        "        if not selected:\n",
        "            i = int(np.argmax(rel[candidates]))\n",
        "            selected.append(candidates[i])\n",
        "            candidates.pop(i)\n",
        "        else:\n",
        "            cand_mat = doc_mat[candidates]\n",
        "            sel_mat = doc_mat[selected]\n",
        "            div = cosine_similarity(cand_mat, sel_mat).max(axis=1)\n",
        "\n",
        "            scores = lambda_mult * rel[candidates] - (1 - lambda_mult) * div\n",
        "            i = int(np.argmax(scores))\n",
        "            selected.append(candidates[i])\n",
        "            candidates.pop(i)\n",
        "\n",
        "\n",
        "    return [(i, float(rel[i])) for i in selected]\n",
        "\n",
        "def generate_answer(query: str, hits: List[Tuple[int, float]]) -> str:\n",
        "    context = \"\\n- \".join(docs[i] for i, _ in hits)\n",
        "    return f\"\"\"Question: {query}\n",
        "\n",
        "Grounded answer (based on retrieved context):\n",
        "{(\"Here is what we found:\\n- \" + context) if hits else \"No supporting passages found.\"}\n",
        "\"\"\"\n",
        "\n",
        "# ==== Try it ====\n",
        "query = \"How does RAG produce better answers than a plain LLM?\"\n",
        "hits = retrieve_topk(query, k=4)\n",
        "hits_mmr = mmr(embed_query_tfidf(query), doc_matrix, lambda_mult=0.7, top_k=3)\n",
        "\n",
        "print(\"Top-k by cosine:\", [(docs[i], round(score, 3)) for i, score in hits], \"\\n\")\n",
        "print(\"MMR re-ranked:\",   [(docs[i], round(score, 3)) for i, score in hits_mmr], \"\\n\")\n",
        "print(generate_answer(query, hits_mmr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Exemple 2 – RAG “from scratch” avec SentenceTransformers + FAISS + llama.cpp\n",
        "\n",
        "Dans cet exemple, on construit un pipeline RAG **à la main**. On découpe d’abord les textes en petits morceaux (chunks), puis on utilise un **modèle d’embeddings de phrases** pour les transformer en vecteurs. Ces vecteurs sont indexés dans **FAISS** pour permettre une recherche rapide. Enfin, on interroge un **LLM local (llama.cpp)** en lui donnant les chunks les plus pertinents comme contexte."
      ],
      "metadata": {
        "id": "nFqIgaAiAsPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download Qwen/Qwen2.5-1.5B-Instruct-GGUF qwen2.5-1.5b-instruct-q5_k_m.gguf --local-dir . --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoQMCPOc6t-x",
        "outputId": "223156ef-214f-44b9-cf8b-deb8e0d4e3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/commands/download.py:141: FutureWarning: Ignoring --local-dir-use-symlinks. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Downloading 'qwen2.5-1.5b-instruct-q5_k_m.gguf' to '.cache/huggingface/download/GmkwVQxx3T_8EJQrCC1D671Wtjg=.b46661073c18e5b56a41fa320975f866a00def1ff08feef4718e013258896f8c.incomplete'\n",
            "qwen2.5-1.5b-instruct-q5_k_m.gguf: 100% 1.29G/1.29G [00:10<00:00, 124MB/s]\n",
            "Download complete. Moving file to qwen2.5-1.5b-instruct-q5_k_m.gguf\n",
            "qwen2.5-1.5b-instruct-q5_k_m.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "docs = [\n",
        "    (\"doc1\", \"RAG (Retrieval-Augmented Generation) combines retrieval over a vector store with a generator LLM to answer questions grounded in your data.\"),\n",
        "    (\"doc2\", \"FAISS is a library for efficient similarity search on dense vectors. It supports IndexFlatIP for cosine-like similarity via normalized vectors.\"),\n",
        "    (\"doc3\", \"Weaviate is a vector database you can run locally with Docker; you can store vectors and metadata and perform hybrid or vector search.\"),\n",
        "    (\"doc4\", \"LangChain provides chains and integrations: text splitters, embedding helpers, vector stores, and retrieval-augmented QA pipelines.\"),\n",
        "    (\"doc5\", \"Llama.cpp lets you run GGUF local models on CPU/GPU. Use a chat-tuned model for best QA quality with RAG.\"),\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = []\n",
        "metas = []\n",
        "for doc_id, text in docs:\n",
        "    for i, chunk in enumerate(text_splitter.split_text(text)):\n",
        "        chunks.append(chunk)\n",
        "        metas.append({\"doc_id\": doc_id, \"chunk_id\": i})\n",
        "\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "emb = embed_model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "\n",
        "d = emb.shape[1]\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(emb)\n",
        "\n",
        "def retrieve(query: str, k: int = 3):\n",
        "    qv = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
        "    scores, idxs = index.search(qv, k)  # (1, k)\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        results.append({\"score\": float(score), \"text\": chunks[idx], \"meta\": metas[idx]})\n",
        "    return results\n",
        "\n",
        "\n",
        "GGUF_PATH = \"/content/qwen2.5-1.5b-instruct-q5_k_m.gguf\"\n",
        "llm = Llama(\n",
        "    model_path=GGUF_PATH,\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=0\n",
        ")\n",
        "\n",
        "def answer_with_rag(query: str, top_k: int = 3):\n",
        "    retrieved = retrieve(query, k=top_k)\n",
        "    context_block = \"\\n\\n\".join(\n",
        "        [f\"[{i+1}] {r['text']}\" for i, r in enumerate(retrieved)]\n",
        "    )\n",
        "    prompt = f\"\"\"You are a helpful expert. Answer the user's question using ONLY the context.\n",
        "If the answer isn't in the context, say you don't know.\n",
        "\n",
        "# Context\n",
        "{context_block}\n",
        "\n",
        "# Question\n",
        "{query}\n",
        "\n",
        "# Answer\"\"\"\n",
        "\n",
        "    out = llm.create_chat_completion(\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=512,\n",
        "    )\n",
        "    return out[\"choices\"][0][\"message\"][\"content\"], retrieved\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    query = \"how does RAG use FAISS and LangChain together?\"\n",
        "    answer, retrieved = answer_with_rag(query, top_k=3)\n",
        "    print(\"Top passages:\")\n",
        "    for r in retrieved:\n",
        "        print(f\"- {r['meta']} | score={r['score']:.3f}\\n  {r['text']}\\n\")\n",
        "    print(\"Answer:\\n\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JQFBzT74YfP",
        "outputId": "68731b92-680a-4211-dc40-26eeee247df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 26 key-value pairs and 339 tensors from /content/qwen2.5-1.5b-instruct-q5_k_m.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = qwen2.5-1.5b-instruct\n",
            "llama_model_loader: - kv   3:                            general.version str              = v0.1\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = qwen2.5-1.5b-instruct\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 1.8B\n",
            "llama_model_loader: - kv   6:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv   7:                       qwen2.context_length u32              = 32768\n",
            "llama_model_loader: - kv   8:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv   9:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  10:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  11:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  12:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  13:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 17\n",
            "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151645\n",
            "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  23:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type q5_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q5_K - Medium\n",
            "print_info: file size   = 1.19 GiB (5.76 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: control token: 151660 '<|fim_middle|>' is not marked as EOG\n",
            "load: control token: 151659 '<|fim_prefix|>' is not marked as EOG\n",
            "load: control token: 151653 '<|vision_end|>' is not marked as EOG\n",
            "load: control token: 151648 '<|box_start|>' is not marked as EOG\n",
            "load: control token: 151646 '<|object_ref_start|>' is not marked as EOG\n",
            "load: control token: 151649 '<|box_end|>' is not marked as EOG\n",
            "load: control token: 151655 '<|image_pad|>' is not marked as EOG\n",
            "load: control token: 151651 '<|quad_end|>' is not marked as EOG\n",
            "load: control token: 151647 '<|object_ref_end|>' is not marked as EOG\n",
            "load: control token: 151652 '<|vision_start|>' is not marked as EOG\n",
            "load: control token: 151654 '<|vision_pad|>' is not marked as EOG\n",
            "load: control token: 151656 '<|video_pad|>' is not marked as EOG\n",
            "load: control token: 151644 '<|im_start|>' is not marked as EOG\n",
            "load: control token: 151661 '<|fim_suffix|>' is not marked as EOG\n",
            "load: control token: 151650 '<|quad_start|>' is not marked as EOG\n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load:   - 151662 ('<|fim_pad|>')\n",
            "load:   - 151663 ('<|repo_name|>')\n",
            "load:   - 151664 ('<|file_sep|>')\n",
            "load: special tokens cache size = 22\n",
            "load: token to piece cache size = 0.9310 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 1000000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.78 B\n",
            "print_info: general.name     = qwen2.5-1.5b-instruct\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOS token        = 151645 '<|im_end|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q5_K) (and 338 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  1220.27 MiB\n",
            "...........................................................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 4096\n",
            "llama_context: n_ctx_per_seq = 4096\n",
            "llama_context: n_batch       = 512\n",
            "llama_context: n_ubatch      = 512\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: kv_unified    = false\n",
            "llama_context: freq_base     = 1000000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.58 MiB\n",
            "create_memory: n_ctx = 4096 (padded)\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   112.00 MiB\n",
            "llama_kv_cache_unified: size =  112.00 MiB (  4096 cells,  28 layers,  1/1 seqs), K (f16):   56.00 MiB, V (f16):   56.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 2712\n",
            "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
            "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
            "llama_context:        CPU compute buffer size =   302.75 MiB\n",
            "llama_context: graph nodes  = 1070\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '17', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'qwen2.embedding_length': '1536', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'qwen2.5-1.5b-instruct', 'qwen2.block_count': '28', 'general.version': 'v0.1', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.finetune': 'qwen2.5-1.5b-instruct', 'general.type': 'model', 'general.size_label': '1.8B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': '{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n', 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '8960', 'qwen2.attention.head_count': '12'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Using gguf chat template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "Using chat eos_token: <|im_end|>\n",
            "Using chat bos_token: <|endoftext|>\n",
            "llama_perf_context_print:        load time =   25000.57 ms\n",
            "llama_perf_context_print: prompt eval time =   24999.99 ms /   170 tokens (  147.06 ms per token,     6.80 tokens per second)\n",
            "llama_perf_context_print:        eval time =    8321.89 ms /    41 runs   (  202.97 ms per token,     4.93 tokens per second)\n",
            "llama_perf_context_print:       total time =   33396.42 ms /   211 tokens\n",
            "llama_perf_context_print:    graphs reused =         39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top passages:\n",
            "- {'doc_id': 'doc1', 'chunk_id': 0} | score=0.370\n",
            "  RAG (Retrieval-Augmented Generation) combines retrieval over a vector store with a generator LLM to answer questions grounded in your data.\n",
            "\n",
            "- {'doc_id': 'doc4', 'chunk_id': 0} | score=0.291\n",
            "  LangChain provides chains and integrations: text splitters, embedding helpers, vector stores, and retrieval-augmented QA pipelines.\n",
            "\n",
            "- {'doc_id': 'doc2', 'chunk_id': 0} | score=0.111\n",
            "  FAISS is a library for efficient similarity search on dense vectors. It supports IndexFlatIP for cosine-like similarity via normalized vectors.\n",
            "\n",
            "Answer:\n",
            " RAG uses FAISS for efficient similarity search on dense vectors, and LangChain provides chains and integrations such as text splitters, embedding helpers, vector stores, and retrieval-augmented QA pipelines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Exemple 3 – RAG avec LangChain + FAISS + LlamaCpp\n",
        "\n",
        "Ici, nous faisons la même chose que dans l’exemple précédent, mais en utilisant **LangChain** pour simplifier le code. LangChain fournit des composants prêts à l’emploi : découpeur de texte, embeddings, vector store FAISS, et chaîne de question-réponse (`RetrievalQA`). Au lieu d’assembler nous-mêmes toutes les étapes, nous les **“câblons” via LangChain**. L’objectif pédagogique est de montrer comment, une fois que vous avez compris le RAG “from scratch”, vous pouvez passer à un framework qui rend le code plus court, plus lisible et plus facile à maintenir.\n",
        "\n"
      ],
      "metadata": {
        "id": "iu9KanhOAzHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "docs = [\n",
        "  \"RAG couples retrieval with generation for grounded answers.\",\n",
        "  \"FAISS performs fast vector similarity search.\",\n",
        "  \"LangChain wires together splitters, embeddings, vector stores, and QA chains.\",\n",
        "]\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "splits = splitter.create_documents(docs)\n",
        "\n",
        "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vs = FAISS.from_documents(splits, emb)\n",
        "\n",
        "llm = LlamaCpp(model_path=\"/content/qwen2.5-1.5b-instruct-q5_k_m.gguf\", n_ctx=4096, temperature=0.2)\n",
        "\n",
        "template = \"\"\"Use only the context to answer. If missing, say you don't know.\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vs.as_retriever(search_kwargs={\"k\":3}),\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "\n",
        "print(qa({\"query\":\"how does FAISS help RAG?\"}))\n"
      ],
      "metadata": {
        "id": "VFQtCpmd93kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Exemple 4 – RAG avec base vectorielle externe (Weaviate + SentenceTransformers + Llama)\n",
        "\n",
        "Dans ce dernier exemple, nous passons à un scénario plus **“production”** en utilisant **Weaviate** comme base de données vectorielle. Les embeddings sont toujours calculés côté Python (SentenceTransformers), mais les vecteurs et les métadonnées sont **stockés et recherchés dans Weaviate**, qui gère l’indexation et la recherche à grande échelle. Le LLM (llama.cpp) reste côté application. Cet exemple illustre la différence entre un petit RAG local (FAISS en mémoire) et un RAG plus industriel basé sur une **base vectorielle distante**, comme on le ferait dans une application réelle."
      ],
      "metadata": {
        "id": "dAjJraloA3gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os, uuid, pathlib\n",
        "import numpy as np\n",
        "import weaviate\n",
        "from weaviate.classes.config import Property, DataType, Configure\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_cpp import Llama\n",
        "\n",
        "\n",
        "WCS_REST_URL = \"https://6ca79jmsuiwhl8eebxug.c0.europe-west3.gcp.weaviate.cloud\"  # from your dashboard\n",
        "WCS_API_KEY  = \"a1VETStBWUxvMVdCOGRXK19ndGovMWZmdkRYQzRUeFByaUl5MTU5NjBWTlV6QldaNjhIZ0lybnB2Sk5zPV92MjAw\"  # <-- your key\n",
        "\n",
        "GGUF_PATH = \"/content/qwen2.5-1.5b-instruct-q5_k_m.gguf\"  # e.g. \"/content/models/llama-3.1-8B-instruct.Q4_K_M.gguf\"\n",
        "\n",
        "# Quick sanity check on model path (helpful in Colab)\n",
        "assert pathlib.Path(GGUF_PATH).exists(), f\"GGUF model not found at: {GGUF_PATH}\"\n",
        "\n",
        "# --- 3) Connect to Weaviate v4 (collections API) ---\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WCS_REST_URL,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(WCS_API_KEY),\n",
        "    headers={\"X-OpenAI-Project\": \"rag-demo-colab\"}\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Connected to Weaviate:\", client.is_connected())\n",
        "\n",
        "COLLECTION = \"Note\"\n",
        "\n",
        "try:\n",
        "    client.collections.delete(COLLECTION)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "notes = client.collections.create(\n",
        "    name=COLLECTION,\n",
        "    # no auto-vectorization; we'll push our own vectors\n",
        "    vectorizer_config=None,\n",
        "    properties=[\n",
        "        Property(name=\"text\", data_type=DataType.TEXT)\n",
        "    ],\n",
        "    vector_index_config=Configure.VectorIndex.hnsw()\n",
        ")\n",
        "\n",
        "print(\"Collection ready:\", notes.name)\n",
        "\n",
        "docs = [\n",
        "    \"RAG ties retrieval over your corpus to an LLM so answers stay grounded.\",\n",
        "    \"FAISS is efficient for similarity search over dense vectors.\",\n",
        "    \"Weaviate is a vector DB: store vectors plus metadata, and query by vector.\",\n",
        "    \"Sentence Transformers turn text into dense vectors that capture semantics.\",\n",
        "    \"In RAG, you retrieve top-k passages and feed them into the LLM as context.\"\n",
        "]\n",
        "\n",
        "embed = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vecs = embed.encode(docs, normalize_embeddings=True)\n",
        "\n",
        "for text, vec in zip(docs, vecs):\n",
        "    notes.data.insert(\n",
        "        properties={\"text\": text},\n",
        "        vector=vec.astype(np.float32)\n",
        "    )\n",
        "\n",
        "print(\"Inserted\", notes.aggregate.over_all().total_count, \"objects\")\n",
        "\n",
        "def retrieve(query: str, k: int = 3):\n",
        "    qv = embed.encode([query], normalize_embeddings=True)[0].astype(np.float32)\n",
        "    res = notes.query.near_vector(\n",
        "        near_vector=qv,\n",
        "        limit=k,\n",
        "        return_properties=[\"text\"],\n",
        "        return_metadata=weaviate.classes.query.MetadataQuery(distance=True)\n",
        "    )\n",
        "    items = [\n",
        "        {\"text\": o.properties[\"text\"], \"distance\": o.metadata.distance}\n",
        "        for o in res.objects\n",
        "    ]\n",
        "    return items\n",
        "\n",
        "# --- 8) Local LLM via llama.cpp ---\n",
        "# Tip: On Colab CPU, keep n_threads modest. If you have Colab T4, you can set n_gpu_layers > 0 if compiled with CUDA.\n",
        "llm = Llama(\n",
        "    model_path=GGUF_PATH,\n",
        "    n_ctx=4096,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=0,     # set >0 only if your wheel supports GPU and your Colab has CUDA\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "def answer_with_llama(context: str, question: str, temperature: float = 0.2, max_tokens: int = 512):\n",
        "    prompt = f\"\"\"You are a precise assistant. Answer using ONLY the context. If insufficient, say \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    # Prefer chat API if the model is chat-tuned; otherwise use .create_completion\n",
        "    out = llm.create_chat_completion(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return out[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "def rag_answer(query: str, k: int = 3):\n",
        "    hits = retrieve(query, k=k)\n",
        "    ctx = \"\\n\\n\".join(f\"[{i+1}] {h['text']}\" for i, h in enumerate(hits))\n",
        "    answer = answer_with_llama(ctx, query)\n",
        "    return answer, hits\n",
        "\n",
        "\n",
        "q = \"What is FAISS used for in a RAG pipeline?\"\n",
        "ans, hits = rag_answer(q, k=3)\n",
        "\n",
        "print(\"Query:\", q)\n",
        "print(\"\\nTop-K retrieved:\")\n",
        "for i, h in enumerate(hits, 1):\n",
        "    print(f\"{i}. (distance={h['distance']:.4f}) {h['text']}\")\n",
        "\n",
        "print(\"\\nAnswer:\\n\", ans)\n"
      ],
      "metadata": {
        "id": "edLNoIyJ9-9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        " Mettre en place un système RAG à partir de fichiers PDF\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A4XZC-R1_HhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy weaviate-client sentence-transformers llama-cpp-python pypdf"
      ],
      "metadata": {
        "id": "4f__tyHB_LK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e359ee1c-98d0-4abb-a4a9-bc6be77ccdbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting weaviate-client\n",
            "  Downloading weaviate_client-4.18.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.12/dist-packages (0.3.16)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.4.0)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (0.28.1)\n",
            "Collecting validators<1.0.0,>=0.34.0 (from weaviate-client)\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client)\n",
            "  Downloading authlib-1.6.5-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (2.12.4)\n",
            "Requirement already satisfied: grpcio<1.80.0,>=1.59.5 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (1.76.0)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from weaviate-client) (6.33.1)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from weaviate-client)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (12.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (3.4.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation<3.0.0,>=2.1.0->weaviate-client) (23.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Downloading weaviate_client-4.18.1-py3-none-any.whl (598 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.1/598.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading authlib-1.6.5-py2.py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.6/243.6 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: validators, deprecation, authlib, weaviate-client\n",
            "Successfully installed authlib-1.6.5 deprecation-2.1.0 validators-0.35.0 weaviate-client-4.18.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_cpp import Llama\n",
        "from pypdf import PdfReader\n",
        "import faiss  # FAISS pour le stockage vectoriel\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION & INITIALISATION\n",
        "# ==========================================\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/CYBERSECURITÉ FONDEMENTS ET PRATIQUES AVANCÉES - Introduction.pdf\"\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "repo_id = \"Qwen/Qwen2.5-1.5B-Instruct-GGUF\"\n",
        "filename = \"qwen2.5-1.5b-instruct-q5_k_m.gguf\"\n",
        "\n",
        "print(\"⏳ Téléchargement du modèle depuis HuggingFace...\")\n",
        "MODEL_PATH = hf_hub_download(repo_id=repo_id, filename=filename)\n",
        "print(\"Modèle téléchargé :\", MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "7M2zIXmnd3Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae6d018-7e29-465d-f38d-61c6fba3a235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Téléchargement du modèle depuis HuggingFace...\n",
            "Modèle téléchargé : /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct-GGUF/snapshots/91cad51170dc346986eccefdc2dd33a9da36ead9/qwen2.5-1.5b-instruct-q5_k_m.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 2. Chargement du modèle d'embedding\n",
        "# ==========================================\n",
        "print(\"⏳ Chargement du modèle d'embedding...\")\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# ==========================================\n",
        "# 3. Chargement du LLM\n",
        "# ==========================================\n",
        "print(\"⏳ Chargement du LLM...\")\n",
        "llm = Llama(\n",
        "    model_path=MODEL_PATH,\n",
        "    n_ctx=2048,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Lecture du PDF\n",
        "# ==========================================\n",
        "print(\"⏳ Lecture du PDF et création des chunks...\")\n",
        "reader = PdfReader(DATA_PATH)\n",
        "texts = [page.extract_text() for page in reader.pages if page.extract_text() is not None]\n",
        "\n",
        "# Fractionnement simple en chunks\n",
        "chunk_size = 500  # caractères\n",
        "chunks = []\n",
        "for t in texts:\n",
        "    for i in range(0, len(t), chunk_size):\n",
        "        chunks.append(t[i:i+chunk_size])\n",
        "\n"
      ],
      "metadata": {
        "id": "drLITuxTe9yp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecea9dfe-ad84-4ba1-940d-cad95679f019"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Chargement du modèle d'embedding...\n",
            "⏳ Chargement du LLM...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏳ Lecture du PDF et création des chunks...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. Embedding et index FAISS\n",
        "# ==========================================\n",
        "print(\"⏳ Création des embeddings et de l'index FAISS...\")\n",
        "vectors = embed_model.encode(chunks, normalize_embeddings=True)\n",
        "dim = vectors.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatIP(dim)  # Inner Product pour similarité cosinus normalisée\n",
        "index.add(np.array(vectors, dtype=np.float32))\n",
        "\n",
        "# ==========================================\n",
        "# 6. Fonction de recherche et réponse\n",
        "# ==========================================\n",
        "def retrieve_faiss(query, k=3):\n",
        "    q_vec = embed_model.encode([query], normalize_embeddings=True).astype(np.float32)\n",
        "    distances, indices = index.search(q_vec, k)\n",
        "    results = [(chunks[i], float(distances[0][j])) for j, i in enumerate(indices[0])]\n",
        "    return results\n",
        "\n",
        "def answer_with_llama(context, question, temperature=0.2, max_tokens=512):\n",
        "    prompt = f\"\"\"You are a precise assistant. Answer using ONLY the context. If insufficient, say \"I don't know\".\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "    out = llm.create_chat_completion(\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return out[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "def rag_answer(query, k=3):\n",
        "    hits = retrieve_faiss(query, k)\n",
        "    ctx = \"\\n\\n\".join(f\"[{i+1}] {h[0]}\" for i, h in enumerate(hits))\n",
        "    answer = answer_with_llama(ctx, query)\n",
        "    return answer, hits\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFGmCO9vfAgn",
        "outputId": "5dabc30c-af32-4961-ed14-2e92d444bcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Création des embeddings et de l'index FAISS...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. Champ de saisie utilisateur\n",
        "# ==========================================\n",
        "while True:\n",
        "    query = input(\"\\nPose ta question (ou tape 'exit' pour quitter) : \")\n",
        "    if query.lower() in ['exit', 'quit']:\n",
        "        print(\"Fin du programme.\")\n",
        "        break\n",
        "\n",
        "    ans, hits = rag_answer(query, k=3)\n",
        "\n",
        "    print(\"\\nTop-K retrieved chunks:\")\n",
        "    for i, (text, score) in enumerate(hits, 1):\n",
        "        print(f\"{i}. (score={score:.4f}) {text[:100]}...\")\n",
        "\n",
        "    print(\"\\nRéponse générée:\\n\", ans)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_PsLWhPsbPS",
        "outputId": "094692cd-e40e-4b9c-e74a-dbea4184eef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top-K retrieved chunks:\n",
            "1. (score=0.7085) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "La Triade CIA vs DAD\n",
            "• Disclosure: est l'exposition d'infor...\n",
            "2. (score=0.6750) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "La Triade CIA et la Non-Répudiation\n",
            "• La non-répudiation, m...\n",
            "3. (score=0.6510) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "La Triade CIA vs DAD\n",
            "16\n",
            "CYBERSECURITE - FONDEMENTS ET PRATI...\n",
            "\n",
            "Réponse générée:\n",
            " La triade CID/CIA, également connue sous le nom de la triade CIA, comprend les trois propriétés fondamentales suivantes :\n",
            "\n",
            "1. Confidentialité (Confidentiality)\n",
            "   - Disclosure: Exposition d'informations sensibles à des personnes non autorisées.\n",
            "   - Alteration: Modification non autorisée des informations.\n",
            "   - Destruction/Denial: Perturbation de l'accès légitime d'un utilisateur autorisé.\n",
            "\n",
            "2. Intégrité (Integrity)\n",
            "   - Alteration: Modification non autorisée des informations.\n",
            "\n",
            "3. Autonomie (Availability)\n",
            "   - Destruction/Denial: Perturbation de l'accès légitime d'un utilisateur autorisé.\n",
            "\n",
            "Top-K retrieved chunks:\n",
            "1. (score=0.3972) . Les événements de refus \n",
            "violent le principe de disponibilité.\n",
            "• Les modèles CIA et DAD sont très ...\n",
            "2. (score=0.3762) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "La Triade CIA et la Non-Répudiation\n",
            "• La non-répudiation, m...\n",
            "3. (score=0.3602) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "La Disponibilité\n",
            "• Garantie l'accès continu aux données, au...\n",
            "\n",
            "Réponse générée:\n",
            " Je ne peux pas fournir une réponse exacte à cette question en utilisant uniquement le contexte fourni, car il manque des détails clés sur les attaques par dictionnaire et par force brute. Le contexte fourni ne contient pas d'informations spécifiques sur ces techniques d'attaque.\n",
            "\n",
            "Top-K retrieved chunks:\n",
            "1. (score=0.5357) . Les événements de refus \n",
            "violent le principe de disponibilité.\n",
            "• Les modèles CIA et DAD sont très ...\n",
            "2. (score=0.4459) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "La Triade CIA et la Non-Répudiation\n",
            "• La non-répudiation, m...\n",
            "3. (score=0.4376) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "Cybersécurité - Les Risques de Brèches de Sécurité\n",
            "• Les in...\n",
            "\n",
            "Réponse générée:\n",
            " Je ne peux pas fournir une réponse précise à partir de ce contexte, car il ne contient pas d'informations sur les étapes recommandées pour la réponse à un incident ou l'analyse forensique après une intrusion. Les informations fournies sont plutôt des définitions de concepts de sécurité informatique.\n",
            "\n",
            "Top-K retrieved chunks:\n",
            "1. (score=0.5417) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "Cybersécurité - Les Risques de Brèches de Sécurité\n",
            "• Les in...\n",
            "2. (score=0.5093) PLAN DU COURS\n",
            "• Chapitre 1 : Menaces et contrôles de \n",
            "sécurité\n",
            "• Chapitre 2 : Cryptographie et techn...\n",
            "3. (score=0.5015) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "Cybersécurité – Risque Opérationnel\n",
            "• Le risque opérationne...\n",
            "\n",
            "Réponse générée:\n",
            " Les incidents de sécurité sont des violations de la confidentialité, de l'intégrité ou de la disponibilité des informations ou des systèmes.\n",
            "\n",
            "Top-K retrieved chunks:\n",
            "1. (score=0.4191) . Les événements de refus \n",
            "violent le principe de disponibilité.\n",
            "• Les modèles CIA et DAD sont très ...\n",
            "2. (score=0.3796) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "Cybersécurité – Risque Opérationnel\n",
            "• Le risque opérationne...\n",
            "3. (score=0.3711) INTRODUCTION A LA SECURITE \n",
            "INFORMATIQUE\n",
            "Cybersécurité – Risque Stratégique\n",
            "• Le risque stratégique ...\n",
            "\n",
            "Réponse générée:\n",
            " Les différents risques mentionnés dans le contexte sont :\n",
            "\n",
            "1. Risque opérationnel\n",
            "2. Risque stratégique\n",
            "\n",
            "Pose ta question (ou tape 'exit' pour quitter) : exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLsowvUntaTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}